{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82c13f0-9765-4f85-ac36-ee6a0f831467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # House Price Prediction - Feature Engineering\n",
    "# MAGIC ## MLflow Production Pipeline - Step 2\n",
    "# MAGIC \n",
    "# MAGIC **Objective**: Transform raw data into ML-ready features\n",
    "# MAGIC \n",
    "# MAGIC **Author**: Satish  \n",
    "# MAGIC **Date**: 2026-01-17\n",
    "# MAGIC \n",
    "# MAGIC ---\n",
    "# MAGIC \n",
    "# MAGIC ### What This Notebook Does:\n",
    "# MAGIC - ‚úÖ Load and validate raw data\n",
    "# MAGIC - ‚úÖ Create derived features\n",
    "# MAGIC - ‚úÖ Encode categorical variables\n",
    "# MAGIC - ‚úÖ Scale numerical features\n",
    "# MAGIC - ‚úÖ Split train/test datasets\n",
    "# MAGIC - ‚úÖ Save processed data and transformers\n",
    "# MAGIC - ‚úÖ Log everything to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d7827c-b900-4487-8de7-b38c2d662f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015d7e6b-ba56-4fed-8479-08556dd75839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad71e7bb-af9d-408e-a3a5-f3b51c5e4ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clear cached modules and reload\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add project path\n",
    "project_path = '/Workspace/COMM - Commercial Analytics (CMAN)/MMM Quattro 2025/Satish/MLFLOW_sample'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "# Remove cached src modules\n",
    "modules_to_clear = [key for key in list(sys.modules.keys()) if key.startswith('src')]\n",
    "for module in modules_to_clear:\n",
    "    del sys.modules[module]\n",
    "\n",
    "print(f\"üîÑ Cleared {len(modules_to_clear)} cached modules\")\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom imports\n",
    "from src.utils import (\n",
    "    ConfigLoader, \n",
    "    DataLoader, \n",
    "    DataValidator,\n",
    "    safe_display,\n",
    "    setup_mlflow_databricks,\n",
    "    MLflowLogger,\n",
    "    log_dataset_summary\n",
    ")\n",
    "from src.data_processing import DataProcessor\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")\n",
    "print(f\"üì¶ MLflow version: {mlflow.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c200fb47-1ef8-4b83-902e-50ba674ba23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Load Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b1f90d-f427-4400-8eda-e7cf244e132a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load configuration\n",
    "config_path = '/Workspace/COMM - Commercial Analytics (CMAN)/MMM Quattro 2025/Satish/MLFLOW_sample/config/config.yaml'\n",
    "config = ConfigLoader.load_config(config_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "print(f\"Version: {config['project']['version']}\")\n",
    "print(f\"Target: {config['preprocessing']['target']}\")\n",
    "print(f\"Test Size: {config['preprocessing']['test_size']}\")\n",
    "print(f\"Random State: {config['preprocessing']['random_state']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14f97cb-59ba-45e4-b8ef-49ac059ec503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Setup MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ec46ff-44a1-40dd-a034-67408d603926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setup MLflow for Databricks\n",
    "setup_mlflow_databricks(config)\n",
    "\n",
    "# Start MLflow run for feature engineering\n",
    "mlflow.start_run(run_name=\"feature_engineering\")\n",
    "\n",
    "print(\"‚úÖ MLflow tracking started\")\n",
    "print(f\"üìä Experiment: {config['mlflow']['experiment_name']}\")\n",
    "print(f\"üîó Run ID: {mlflow.active_run().info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0422e745-f67a-422b-85e0-e9a2b19ed5a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105133cb-83a1-46cb-86eb-15c71ed55446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "data_path = config['data']['raw_path']\n",
    "df = DataLoader.load_csv(data_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAW DATA LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "safe_display(df.head())\n",
    "\n",
    "print(\"\\nüìä Data types:\")\n",
    "print(df.dtypes.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83fba8f-02c9-45bd-a611-bed58906a277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Initialize Processors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881d4305-35c2-4fca-bdf3-6dca9812e47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize data processor and feature engineer\n",
    "processor = DataProcessor(config)\n",
    "feature_engineer = FeatureEngineer(config)\n",
    "\n",
    "print(\"‚úÖ DataProcessor initialized\")\n",
    "print(\"‚úÖ FeatureEngineer initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94d4b23-1b53-4e65-8d56-55034db19f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a25719e-449b-4eb4-8010-8d20b5007ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Validate data\n",
    "print(\"=\"*60)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "validator = DataValidator()\n",
    "\n",
    "# Check required columns\n",
    "all_required_cols = (\n",
    "    config['preprocessing']['numerical_features'] + \n",
    "    config['preprocessing']['categorical_features'] + \n",
    "    [config['preprocessing']['target']]\n",
    ")\n",
    "\n",
    "try:\n",
    "    validator.validate_dataframe(df, all_required_cols)\n",
    "    print(\"‚úÖ All required columns present\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check missing values\n",
    "missing = validator.check_missing_values(df)\n",
    "mlflow.log_metric(\"raw_missing_values\", int(missing.sum()))\n",
    "\n",
    "# Check duplicates\n",
    "duplicates = validator.check_duplicates(df)\n",
    "mlflow.log_metric(\"raw_duplicates\", int(duplicates))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02686085-e55e-4064-8437-937e97285c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84806ed-8de6-4c8d-ae1f-73cdb9332896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 7.1 Create New Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d18ba5-d6b4-43e8-b446-7257f3a2d7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING NEW FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create features\n",
    "df_engineered = feature_engineer.create_features(df)\n",
    "\n",
    "# Show new features\n",
    "new_features = set(df_engineered.columns) - set(df.columns)\n",
    "print(f\"\\n‚úÖ Created {len(new_features)} new features:\")\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(f\"\\nüìä New shape: {df_engineered.shape[0]} rows √ó {df_engineered.shape[1]} columns\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample with new features:\")\n",
    "safe_display(df_engineered.head())\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_metric(\"features_created\", len(new_features))\n",
    "mlflow.log_metric(\"total_features\", df_engineered.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a3c13b-b1c8-4254-af19-dea68bc7953b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 7.2 Analyze New Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c18741-3225-4a39-91d8-32f0dcfa337c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analyze new features\n",
    "print(\"=\"*60)\n",
    "print(\"NEW FEATURES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target = config['preprocessing']['target']\n",
    "\n",
    "# Calculate correlations with target\n",
    "new_feature_list = list(new_features)\n",
    "if new_feature_list:\n",
    "    correlations = {}\n",
    "    for feat in new_feature_list:\n",
    "        if pd.api.types.is_numeric_dtype(df_engineered[feat]):\n",
    "            corr = df_engineered[feat].corr(df_engineered[target])\n",
    "            correlations[feat] = corr\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(\"\\nüîó Correlation with target:\")\n",
    "    for feat, corr in sorted_corr:\n",
    "        print(f\"  {feat}: {corr:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    if sorted_corr:\n",
    "        fig, axes = plt.subplots(1, len(sorted_corr), figsize=(6*len(sorted_corr), 5))\n",
    "        if len(sorted_corr) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (feat, corr) in enumerate(sorted_corr):\n",
    "            axes[idx].scatter(df_engineered[feat], df_engineered[target], \n",
    "                            alpha=0.6, color='purple', edgecolors='black', linewidth=0.5)\n",
    "            axes[idx].set_xlabel(feat, fontsize=11)\n",
    "            axes[idx].set_ylabel(target, fontsize=11)\n",
    "            axes[idx].set_title(f'{feat} vs {target}\\n(r = {corr:.3f})', \n",
    "                              fontsize=12, fontweight='bold')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(df_engineered[feat], df_engineered[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[idx].plot(df_engineered[feat], p(df_engineered[feat]), \n",
    "                          \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
    "            axes[idx].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Log top correlation\n",
    "    if sorted_corr:\n",
    "        mlflow.log_metric(\"top_new_feature_corr\", abs(sorted_corr[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfe2223-c9fb-4aba-82c6-1329ed8d7677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 7.3 Feature Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a661b3e-a24e-4a0e-939b-729437e00f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Statistics for new features\n",
    "print(\"=\"*60)\n",
    "print(\"NEW FEATURES STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feat in new_feature_list:\n",
    "    if pd.api.types.is_numeric_dtype(df_engineered[feat]):\n",
    "        print(f\"\\n{feat}:\")\n",
    "        print(f\"  Mean:     {df_engineered[feat].mean():.2f}\")\n",
    "        print(f\"  Median:   {df_engineered[feat].median():.2f}\")\n",
    "        print(f\"  Std:      {df_engineered[feat].std():.2f}\")\n",
    "        print(f\"  Min:      {df_engineered[feat].min():.2f}\")\n",
    "        print(f\"  Max:      {df_engineered[feat].max():.2f}\")\n",
    "        print(f\"  Skewness: {df_engineered[feat].skew():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24eefa5-eb0f-4dd0-b666-cd958605cf10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Encode Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595abb98-2d05-4f5b-9894-c27d75519e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Encode Categorical Features\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Encode categorical features - USE feature_engineer, NOT processor!\n",
    "df_encoded, label_encoders = feature_engineer.encode_categorical(df_engineered)\n",
    "\n",
    "print(f\"\\n‚úÖ Encoded {len(label_encoders)} categorical features:\")\n",
    "for col, encoder in label_encoders.items():\n",
    "    print(f\"\\n  üìå {col}:\")\n",
    "    print(f\"    Classes: {list(encoder.classes_)}\")\n",
    "    mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    for original, encoded in mapping.items():\n",
    "        print(f\"      {original} ‚Üí {encoded}\")\n",
    "\n",
    "# Show before/after\n",
    "print(\"\\nüìã Before encoding:\")\n",
    "safe_display(df_engineered[config['preprocessing']['categorical_features']].head())\n",
    "\n",
    "print(\"\\nüìã After encoding:\")\n",
    "safe_display(df_encoded[config['preprocessing']['categorical_features']].head())\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_metric(\"categorical_features_encoded\", len(label_encoders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98ceedf-f8e3-40e8-97f0-b1ea216657ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1428aabd-3920-4374-b484-8d23a72d31b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Train-Test Split\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = processor.split_data(df_encoded)\n",
    "\n",
    "print(f\"\\nüìä Training set:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_train range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nüìä Test set:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"  y_test range: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nüìà Split ratio:\")\n",
    "print(f\"  Train: {len(X_train) / len(df_encoded) * 100:.1f}%\")\n",
    "print(f\"  Test:  {len(X_test) / len(df_encoded) * 100:.1f}%\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(y_train, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Training Set - Target Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel(config['preprocessing']['target'], fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(y_test, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('Test Set - Target Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel(config['preprocessing']['target'], fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_params({\n",
    "    \"train_size\": len(X_train),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"test_ratio\": config['preprocessing']['test_size'],\n",
    "    \"random_state\": config['preprocessing']['random_state']\n",
    "})\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"y_train_mean\": float(y_train.mean()),\n",
    "    \"y_train_std\": float(y_train.std()),\n",
    "    \"y_test_mean\": float(y_test.mean()),\n",
    "    \"y_test_std\": float(y_test.std())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df147f4d-d2b0-4e26-b952-6a6c641ce0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab908776-e098-4c5b-936e-ed5e137f378c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c760224c-c7d2-409e-a206-4039307d78c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_test_scaled, scaler = processor.scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Scaled {X_train_scaled.shape[1]} features using StandardScaler\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(\"\\nüìä Scaling parameters (first 5 features):\")\n",
    "for i, col in enumerate(X_train.columns[:5]):\n",
    "    print(f\"  {col}:\")\n",
    "    print(f\"    Mean:  {scaler.mean_[i]:>10.4f}\")\n",
    "    print(f\"    Scale: {scaler.scale_[i]:>10.4f}\")\n",
    "\n",
    "# Compare before/after for first 3 features\n",
    "print(\"\\nüìà Scaling effect (first 3 features):\")\n",
    "for i, col in enumerate(X_train.columns[:3]):\n",
    "    print(f\"\\n  {col}:\")\n",
    "    print(f\"    Before: mean={X_train[col].mean():>8.2f}, std={X_train[col].std():>8.2f}\")\n",
    "    print(f\"    After:  mean={X_train_scaled[col].mean():>8.2f}, std={X_train_scaled[col].std():>8.2f}\")\n",
    "\n",
    "# Visualize scaling effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before scaling\n",
    "box_data_before = [X_train[col] for col in X_train.columns[:5]]\n",
    "axes[0].boxplot(box_data_before, labels=X_train.columns[:5])\n",
    "axes[0].set_title('Before Scaling (First 5 Features)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# After scaling\n",
    "box_data_after = [X_train_scaled[col] for col in X_train_scaled.columns[:5]]\n",
    "axes[1].boxplot(box_data_after, labels=X_train_scaled.columns[:5])\n",
    "axes[1].set_title('After Scaling (First 5 Features)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Scaled Value', fontsize=11)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_param(\"scaling_method\", \"StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c656a2-c63e-4ef8-831d-0e0ad0f42a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Final Feature Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aeb641-2ff0-452a-9026-e3f085e55db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL FEATURE SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Total features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\nüìã Feature list:\")\n",
    "for i, col in enumerate(X_train_scaled.columns, 1):\n",
    "    is_new = \"üÜï\" if col in new_features else \"  \"\n",
    "    print(f\"  {i:2d}. {is_new} {col}\")\n",
    "\n",
    "# Feature types breakdown\n",
    "numerical_features = config['preprocessing']['numerical_features']\n",
    "categorical_features = config['preprocessing']['categorical_features']\n",
    "\n",
    "print(f\"\\nüìà Feature breakdown:\")\n",
    "print(f\"  Original numerical:   {len(numerical_features)}\")\n",
    "print(f\"  Original categorical: {len(categorical_features)}\")\n",
    "print(f\"  Engineered features:  {len(new_features)}\")\n",
    "print(f\"  {'‚îÄ' * 40}\")\n",
    "print(f\"  Total:                {X_train_scaled.shape[1]}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Training data sample (scaled):\")\n",
    "safe_display(X_train_scaled.head())\n",
    "\n",
    "print(\"\\nüìã Target variable sample:\")\n",
    "safe_display(y_train.head().to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1018564-b63d-4453-b085-c9129c010a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 12. Data Quality Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6f9619-6a5e-4fcb-b2b3-e8b5663e69d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSED DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for any issues in processed data\n",
    "print(\"\\nüîç Quality checks:\")\n",
    "\n",
    "# Missing values\n",
    "missing_train = X_train_scaled.isnull().sum().sum()\n",
    "missing_test = X_test_scaled.isnull().sum().sum()\n",
    "print(f\"  Missing values (train): {missing_train}\")\n",
    "print(f\"  Missing values (test):  {missing_test}\")\n",
    "\n",
    "# Infinite values\n",
    "inf_train = np.isinf(X_train_scaled.values).sum()\n",
    "inf_test = np.isinf(X_test_scaled.values).sum()\n",
    "print(f\"  Infinite values (train): {inf_train}\")\n",
    "print(f\"  Infinite values (test):  {inf_test}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nüìã Data types:\")\n",
    "print(X_train_scaled.dtypes.value_counts().to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Summary statistics (train):\")\n",
    "summary_stats = X_train_scaled.describe()\n",
    "safe_display(summary_stats)\n",
    "\n",
    "# Log quality metrics\n",
    "mlflow.log_metrics({\n",
    "    \"processed_missing_train\": int(missing_train),\n",
    "    \"processed_missing_test\": int(missing_test),\n",
    "    \"processed_inf_train\": int(inf_train),\n",
    "    \"processed_inf_test\": int(inf_test)\n",
    "})\n",
    "\n",
    "if missing_train == 0 and missing_test == 0 and inf_train == 0 and inf_test == 0:\n",
    "    print(\"\\n‚úÖ Data quality: EXCELLENT - Ready for modeling!\")\n",
    "    mlflow.log_param(\"data_quality\", \"EXCELLENT\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Data quality issues detected - review before modeling\")\n",
    "    mlflow.log_param(\"data_quality\", \"ISSUES_DETECTED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ab068b-8ca2-4d60-aeaa-829ac03ea31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 13. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594b86f1-3217-4b48-82c1-af72b6fd87f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Save Processed Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_path = config['data']['processed_path']\n",
    "\n",
    "# Ensure processed directory exists\n",
    "import os\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "from src.utils import DataLoader\n",
    "\n",
    "try:\n",
    "    DataLoader.save_csv(X_train_scaled, f'{processed_path}X_train.csv')\n",
    "    DataLoader.save_csv(X_test_scaled, f'{processed_path}X_test.csv')\n",
    "    DataLoader.save_csv(y_train.to_frame(), f'{processed_path}y_train.csv')\n",
    "    DataLoader.save_csv(y_test.to_frame(), f'{processed_path}y_test.csv')\n",
    "    \n",
    "    print(\"\\n‚úÖ Datasets saved:\")\n",
    "    print(f\"  ‚Ä¢ X_train.csv: {X_train_scaled.shape}\")\n",
    "    print(f\"  ‚Ä¢ X_test.csv:  {X_test_scaled.shape}\")\n",
    "    print(f\"  ‚Ä¢ y_train.csv: {y_train.shape}\")\n",
    "    print(f\"  ‚Ä¢ y_test.csv:  {y_test.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving datasets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save preprocessors\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    # Save label encoders\n",
    "    with open(f'{processed_path}label_encoders.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "    print(\"\\n‚úÖ Label encoders saved\")\n",
    "    \n",
    "    # Save scaler\n",
    "    with open(f'{processed_path}scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"‚úÖ Scaler saved\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving preprocessors: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save feature names (ensure all are JSON-serializable)\n",
    "try:\n",
    "    # Helper function to convert to list\n",
    "    def to_list(obj):\n",
    "        if isinstance(obj, (set, tuple)):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return obj\n",
    "        else:\n",
    "            return [obj]\n",
    "    \n",
    "    feature_names = {\n",
    "        'all_features': list(X_train_scaled.columns),\n",
    "        'created_features': to_list(new_features),\n",
    "        'numerical_features': to_list(numerical_features),\n",
    "        'categorical_features': to_list(categorical_features),\n",
    "        'target': config['preprocessing']['target'],\n",
    "        'total_features': len(X_train_scaled.columns),\n",
    "        'train_samples': len(X_train_scaled),\n",
    "        'test_samples': len(X_test_scaled)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(f'{processed_path}feature_names.json', 'w') as f:\n",
    "        json.dump(feature_names, f, indent=2)\n",
    "    print(\"‚úÖ Feature names saved\")\n",
    "    \n",
    "    # Display saved feature info\n",
    "    print(f\"\\nüìã Feature Information:\")\n",
    "    print(f\"  ‚Ä¢ Total features: {feature_names['total_features']}\")\n",
    "    print(f\"  ‚Ä¢ Created features: {len(feature_names['created_features'])}\")\n",
    "    print(f\"  ‚Ä¢ Training samples: {feature_names['train_samples']}\")\n",
    "    print(f\"  ‚Ä¢ Test samples: {feature_names['test_samples']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving feature names: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüìÅ All files saved to: {processed_path}\")\n",
    "\n",
    "# Log artifacts to MLflow\n",
    "try:\n",
    "    mlflow.log_artifact(f'{processed_path}feature_names.json')\n",
    "    mlflow.log_artifact(f'{processed_path}label_encoders.pkl')\n",
    "    mlflow.log_artifact(f'{processed_path}scaler.pkl')\n",
    "    \n",
    "    # Log summary metrics\n",
    "    mlflow.log_params({\n",
    "        'total_features': feature_names['total_features'],\n",
    "        'created_features_count': len(feature_names['created_features']),\n",
    "        'train_samples': feature_names['train_samples'],\n",
    "        'test_samples': feature_names['test_samples']\n",
    "    })\n",
    "    \n",
    "    print(\"\\n‚úÖ Artifacts logged to MLflow\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not log to MLflow: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2da19c7-ea22-42ba-9a78-38eecc1dc2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 12. Data Quality Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a29cc0-730c-4944-9ca3-18129de4a47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSED DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for any issues in processed data\n",
    "print(\"\\nüîç Quality checks:\")\n",
    "\n",
    "# Missing values\n",
    "missing_train = X_train_scaled.isnull().sum().sum()\n",
    "missing_test = X_test_scaled.isnull().sum().sum()\n",
    "print(f\"  Missing values (train): {missing_train}\")\n",
    "print(f\"  Missing values (test):  {missing_test}\")\n",
    "\n",
    "# Infinite values\n",
    "inf_train = np.isinf(X_train_scaled.values).sum()\n",
    "inf_test = np.isinf(X_test_scaled.values).sum()\n",
    "print(f\"  Infinite values (train): {inf_train}\")\n",
    "print(f\"  Infinite values (test):  {inf_test}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nüìã Data types:\")\n",
    "print(X_train_scaled.dtypes.value_counts().to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Summary statistics (train):\")\n",
    "summary_stats = X_train_scaled.describe()\n",
    "safe_display(summary_stats)\n",
    "\n",
    "# Log quality metrics\n",
    "mlflow.log_metrics({\n",
    "    \"processed_missing_train\": int(missing_train),\n",
    "    \"processed_missing_test\": int(missing_test),\n",
    "    \"processed_inf_train\": int(inf_train),\n",
    "    \"processed_inf_test\": int(inf_test)\n",
    "})\n",
    "\n",
    "if missing_train == 0 and missing_test == 0 and inf_train == 0 and inf_test == 0:\n",
    "    print(\"\\n‚úÖ Data quality: EXCELLENT - Ready for modeling!\")\n",
    "    mlflow.log_param(\"data_quality\", \"EXCELLENT\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Data quality issues detected - review before modeling\")\n",
    "    mlflow.log_param(\"data_quality\", \"ISSUES_DETECTED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c742a4a-1372-48d6-82af-c50e159523c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 13. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af384bb0-27a0-439b-a28f-ac8793a8310c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_path = config['data']['processed_path']\n",
    "\n",
    "# Save datasets\n",
    "DataLoader.save_csv(X_train_scaled, f'{processed_path}X_train.csv')\n",
    "DataLoader.save_csv(X_test_scaled, f'{processed_path}X_test.csv')\n",
    "DataLoader.save_csv(y_train.to_frame(), f'{processed_path}y_train.csv')\n",
    "DataLoader.save_csv(y_test.to_frame(), f'{processed_path}y_test.csv')\n",
    "\n",
    "print(\"\\n‚úÖ Datasets saved:\")\n",
    "print(f\"  ‚Ä¢ X_train.csv: {X_train_scaled.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test.csv:  {X_test_scaled.shape}\")\n",
    "print(f\"  ‚Ä¢ y_train.csv: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ y_test.csv:  {y_test.shape}\")\n",
    "\n",
    "# Save preprocessors\n",
    "import pickle\n",
    "\n",
    "# Save label encoders\n",
    "with open(f'{processed_path}label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(\"\\n‚úÖ Label encoders saved\")\n",
    "\n",
    "# Save scaler\n",
    "with open(f'{processed_path}scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"‚úÖ Scaler saved\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = {\n",
    "    'all_features': list(X_train_scaled.columns),\n",
    "    'created_features': new_features,\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features\n",
    "}\n",
    "import json\n",
    "with open(f'{processed_path}feature_names.json', 'w') as f:\n",
    "    json.dump(list(feature_names), f, indent=2)\n",
    "print(\"‚úÖ Feature names saved\")\n",
    "\n",
    "print(f\"\\nüìÅ All files saved to: {processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0bf95e-f569-4188-ba3e-82729d66e165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 14. Log Artifacts to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b19bb47c-e5c7-4112-8361-081819fe012a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378d8882-7f12-44f6-ad1b-0660984c05ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from src.utils import MLflowLogger as mlflow_logger\n",
    "# MAGIC %md\n",
    "# MAGIC ## 12. Log to MLflow\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOGGING TO MLFLOW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Log datasets as artifacts\n",
    "mlflow_logger.log_dataframe_as_artifact(X_train_scaled, \"X_train.csv\")\n",
    "mlflow_logger.log_dataframe_as_artifact(X_test_scaled, \"X_test.csv\")\n",
    "mlflow_logger.log_dataframe_as_artifact(y_train.to_frame(), \"y_train.csv\")\n",
    "mlflow_logger.log_dataframe_as_artifact(y_test.to_frame(), \"y_test.csv\")\n",
    "\n",
    "print(\"‚úÖ Datasets logged to MLflow\")\n",
    "\n",
    "# Log feature names\n",
    "feature_info = {\n",
    "    \"total_features\": int(X_train_scaled.shape[1]),\n",
    "    \"feature_names\": list(X_train_scaled.columns),\n",
    "    \"numerical_features\": list(numerical_features) if isinstance(numerical_features, set) else numerical_features,\n",
    "    \"categorical_features\": list(categorical_features) if isinstance(categorical_features, set) else categorical_features,\n",
    "    \"engineered_features\": list(new_features) if isinstance(new_features, set) else new_features\n",
    "}\n",
    "mlflow_logger.log_artifact_from_dict(feature_info, \"feature_info.json\")\n",
    "\n",
    "print(\"‚úÖ Feature info logged to MLflow\")\n",
    "\n",
    "# Log preprocessing info\n",
    "preprocessing_info = {\n",
    "    \"label_encoders\": {k: list(v.classes_) for k, v in label_encoders.items()},\n",
    "    \"scaler_mean\": scaler.mean_.tolist(),\n",
    "    \"scaler_scale\": scaler.scale_.tolist(),\n",
    "    \"train_size\": int(len(X_train)),\n",
    "    \"test_size\": int(len(X_test))\n",
    "}\n",
    "mlflow_logger.log_artifact_from_dict(preprocessing_info, \"preprocessing_info.json\")\n",
    "\n",
    "print(\"‚úÖ Preprocessing info logged to MLflow\")\n",
    "\n",
    "# Log feature statistics\n",
    "# feature_stats_dict = feature_stats.to_dict('records')\n",
    "# mlflow_logger.log_artifact_from_dict({\"feature_statistics\": feature_stats_dict}, \"feature_statistics.json\")\n",
    "\n",
    "print(\"‚úÖ Feature statistics logged to MLflow\")\n",
    "\n",
    "print(\"\\nüìä All artifacts logged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a7aaa2-f1c9-406a-a140-259387901dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log feature names\n",
    "feature_info = {\n",
    "    \"total_features\": int(X_train_scaled.shape[1]),\n",
    "    \"feature_names\": list(X_train_scaled.columns),\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"engineered_features\": new_features\n",
    "}\n",
    "mlflow_logger.log_artifact_from_dict(feature_info, \"feature_info.json\")\n",
    "\n",
    "print(\"‚úÖ Feature info logged to MLflow\")\n",
    "\n",
    "# Log preprocessing info\n",
    "preprocessing_info = {\n",
    "    \"label_encoders\": {k: list(v.classes_) for k, v in label_encoders.items()},\n",
    "    \"scaler_mean\": scaler.mean_.tolist(),\n",
    "    \"scaler_scale\": scaler.scale_.tolist(),\n",
    "    \"train_size\": int(len(X_train)),\n",
    "    \"test_size\": int(len(X_test))\n",
    "}\n",
    "mlflow_logger.log_artifact_from_dict(preprocessing_info, \"preprocessing_info.json\")\n",
    "\n",
    "print(\"‚úÖ Preprocessing info logged to MLflow\")\n",
    "\n",
    "print(\"‚úÖ Feature statistics logged to MLflow\")\n",
    "\n",
    "print(\"\\nüìä All artifacts logged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eaee9af-dd68-45ca-b4f2-a621bd122217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 15. Feature Engineering Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d17e8fa-4bd5-44e9-be1f-3f7935e6633d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä DATA TRANSFORMATION COMPLETE\n",
    "\n",
    "INPUT:\n",
    "  ‚Ä¢ Raw data: {df.shape[0]} rows √ó {df.shape[1]} columns\n",
    "  ‚Ä¢ Numerical features: {len(numerical_features)}\n",
    "  ‚Ä¢ Categorical features: {len(categorical_features)}\n",
    "\n",
    "TRANSFORMATIONS:\n",
    "  ‚úì Created {len(new_features)} engineered features\n",
    "  ‚úì Encoded {len(label_encoders)} categorical variables\n",
    "  ‚úì Scaled all numerical features\n",
    "  ‚úì Split into train/test sets\n",
    "\n",
    "OUTPUT:\n",
    "  ‚Ä¢ Training set: {X_train_scaled.shape[0]} samples √ó {X_train_scaled.shape[1]} features\n",
    "  ‚Ä¢ Test set: {X_test_scaled.shape[0]} samples √ó {X_test_scaled.shape[1]} features\n",
    "  ‚Ä¢ Target variable: {target}\n",
    "\n",
    "DATA QUALITY:\n",
    "  ‚úì Missing values: {missing_train + missing_test}\n",
    "  ‚úì Infinite values: {inf_train + inf_test}\n",
    "  ‚úì Data types: All numeric\n",
    "  ‚úì Scaling: StandardScaler applied\n",
    "\n",
    "SAVED FILES:\n",
    "  üìÅ {processed_path}\n",
    "    ‚Ä¢ X_train.csv\n",
    "    ‚Ä¢ X_test.csv\n",
    "    ‚Ä¢ y_train.csv\n",
    "    ‚Ä¢ y_test.csv\n",
    "    ‚Ä¢ label_encoders.pkl\n",
    "    ‚Ä¢ scaler.pkl\n",
    "    ‚Ä¢ feature_names.json\n",
    "\n",
    "MLFLOW:\n",
    "  ‚úì All metrics logged\n",
    "  ‚úì All parameters logged\n",
    "  ‚úì All artifacts logged\n",
    "  üîó Run ID: {mlflow.active_run().info.run_id}\n",
    "\n",
    "STATUS: ‚úÖ READY FOR MODEL TRAINING\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4018e93-d575-4e4c-9cdd-8a24547f6985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 16. Next Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0510e3f-053d-493a-9ac9-9d5b26d76eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ READY FOR MODEL TRAINING (Notebook 03)\n",
    "\n",
    "The processed data is now ready for:\n",
    "  1. ‚úÖ Model training with multiple algorithms\n",
    "  2. ‚úÖ Hyperparameter tuning\n",
    "  3. ‚úÖ Model evaluation and comparison\n",
    "  4. ‚úÖ Model selection and registration\n",
    "  5. ‚úÖ Model deployment\n",
    "\n",
    "Recommended models to try:\n",
    "  ‚Ä¢ Linear Regression (baseline)\n",
    "  ‚Ä¢ Ridge Regression\n",
    "  ‚Ä¢ Lasso Regression\n",
    "  ‚Ä¢ Random Forest Regressor\n",
    "  ‚Ä¢ Gradient Boosting Regressor\n",
    "  ‚Ä¢ XGBoost (if available)\n",
    "\n",
    "All data and transformers are saved and logged to MLflow!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd747c4e-fadc-49c0-a25b-682b392a34b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 17. End MLflow Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2815cab-f3d1-4991-bedc-d6000c72d85d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä MLflow run ended successfully\")\n",
    "print(\"üéØ Ready for Model Training (Notebook 03)\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a14d456-3767-4692-9745-924f77542b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
