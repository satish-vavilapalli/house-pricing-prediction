{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072c34fe-75e1-479d-b28f-a03ab6d5de07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC # 08 - Batch Predictions Pipeline\n",
    "# MAGIC \n",
    "# MAGIC **Generate predictions on new data using the production model**\n",
    "# MAGIC \n",
    "# MAGIC ## Objectives:\n",
    "# MAGIC - Load production model from MLflow\n",
    "# MAGIC - Process new/unseen data\n",
    "# MAGIC - Apply same preprocessing as training\n",
    "# MAGIC - Generate predictions\n",
    "# MAGIC - Save results with confidence intervals\n",
    "# MAGIC - Generate prediction reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8826383-a57b-4389-9505-ec224ad6035d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfd4647-326f-4173-bb45-f0d4790d8e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure clean imports\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32e2be1-d628-4dd8-83b9-cfa9830bed1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6beda0a2-c386-4c8e-bb22-a7fc5264e000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 2. Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f511cd9-604a-49bc-a3c0-98f5e01a624d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BATCH PREDICTIONS PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define project root\n",
    "project_root = \"/Workspace/COMM - Commercial Analytics (CMAN)/MMM Quattro 2025/Satish/MLFLOW_sample\"\n",
    "\n",
    "# Add to path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"\\nüìÇ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Added to sys.path\")\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils import ConfigLoader, DataLoader, safe_display\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "\n",
    "print(f\"‚úÖ Custom modules imported\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9561cf5f-7054-43b9-8737-867e6b827a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e820c2c-dec2-44e1-91ad-aadf014524e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "config_path = f'{project_root}/config/config.yaml'\n",
    "config = ConfigLoader.load_config(config_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration loaded\")\n",
    "print(f\"  ‚Ä¢ Project: {config['project']['name']}\")\n",
    "print(f\"  ‚Ä¢ Model Registry: {config['mlflow']['model_registry_name']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf06790-f3a7-4a5d-af76-bfa356ef33c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 4. Setup MLflow & Load Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46746a73-f497-4a01-9b10-5351bfbfc607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING PRODUCTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "experiment_name = config['mlflow']['experiment_name']\n",
    "model_registry_name = config['mlflow']['model_registry_name']\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "client = MlflowClient()\n",
    "\n",
    "# Load production model\n",
    "model_uri = f\"models:/{model_registry_name}/Production\"\n",
    "\n",
    "try:\n",
    "    production_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Production model loaded\")\n",
    "    print(f\"  ‚Ä¢ Model: {model_registry_name}\")\n",
    "    print(f\"  ‚Ä¢ Stage: Production\")\n",
    "    print(f\"  ‚Ä¢ Type: {type(production_model).__name__}\")\n",
    "    \n",
    "    # Get model version info\n",
    "    model_versions = client.search_model_versions(f\"name='{model_registry_name}'\")\n",
    "    production_version = [v for v in model_versions if v.current_stage == 'Production'][0]\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Version: {production_version.version}\")\n",
    "    print(f\"  ‚Ä¢ Created: {datetime.fromtimestamp(production_version.creation_timestamp / 1000).strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    # Get model metrics\n",
    "    run = client.get_run(production_version.run_id)\n",
    "    metrics = run.data.metrics\n",
    "    \n",
    "    print(f\"\\nüìä Model Performance:\")\n",
    "    print(f\"  ‚Ä¢ RMSE: ${metrics.get('test_rmse', 0):,.2f}\")\n",
    "    print(f\"  ‚Ä¢ MAE:  ${metrics.get('test_mae', 0):,.2f}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤:   {metrics.get('test_r2', 0):.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading production model: {e}\")\n",
    "    print(f\"  ‚Ä¢ Make sure a model is in Production stage\")\n",
    "    raise\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c3c303-94f3-40ab-bef4-3ae32816dd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 5. Load Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ff794-04c2-41ea-ada8-97b92a337bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING PREPROCESSING OBJECTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_path = config['data']['processed_path']\n",
    "\n",
    "try:\n",
    "    # Load label encoders\n",
    "    with open(f\"{processed_path}label_encoders.pkl\", 'rb') as f:\n",
    "        label_encoders = pickle.load(f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Label encoders loaded\")\n",
    "    print(f\"  ‚Ä¢ Categorical features: {len(label_encoders)}\")\n",
    "    \n",
    "    # Load scaler\n",
    "    with open(f\"{processed_path}scaler.pkl\", 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Scaler loaded\")\n",
    "    print(f\"  ‚Ä¢ Feature count: {len(scaler.feature_names_in_)}\")\n",
    "    \n",
    "    print(f\"\\nüìã Expected Features:\")\n",
    "    for i, feature in enumerate(scaler.feature_names_in_, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading preprocessing objects: {e}\")\n",
    "    print(f\"  ‚Ä¢ Make sure you've run the feature engineering notebook\")\n",
    "    raise\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fbe679-0e7b-4ea7-961b-09013e06c5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 6. Load New Data for Predictions\n",
    "# MAGIC \n",
    "# MAGIC **Note:** This example uses the test set. In production, you would load new unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e7701c-b9db-4ccc-8491-5bbd453aac51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING NEW DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Option 1: Load from test set (for demonstration)\n",
    "# In production, replace this with your new data source\n",
    "\n",
    "print(f\"\\nüìä Loading data for predictions...\")\n",
    "print(f\"  ‚Ä¢ Source: Test set (for demonstration)\")\n",
    "\n",
    "# Load raw data\n",
    "raw_data_path = config['data']['raw_path']\n",
    "df_raw = pd.read_csv(raw_data_path)\n",
    "\n",
    "# For demonstration, we'll use a sample of the data\n",
    "# In production, this would be your new data\n",
    "new_data = df_raw.sample(n=min(100, len(df_raw)), random_state=42).copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded\")\n",
    "print(f\"  ‚Ä¢ Records: {len(new_data)}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(new_data.columns)}\")\n",
    "\n",
    "print(f\"\\nüìã Data Preview:\")\n",
    "safe_display(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4e6da5-f259-455c-b90c-cb9371ed3517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 7. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d970c072-26df-4d1b-b412-e808837829bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VALIDATING NEW DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for required columns\n",
    "target_col = config['preprocessing']['target']\n",
    "required_features = [col for col in df_raw.columns if col != target_col]\n",
    "\n",
    "missing_features = [col for col in required_features if col not in new_data.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ùå Missing required features:\")\n",
    "    for feature in missing_features:\n",
    "        print(f\"  ‚Ä¢ {feature}\")\n",
    "    raise ValueError(\"Missing required features\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All required features present\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = new_data[required_features].isnull().sum()\n",
    "missing_features = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing values detected:\")\n",
    "    for feature, count in missing_features.items():\n",
    "        print(f\"  ‚Ä¢ {feature}: {count} ({count/len(new_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîß Handling missing values...\")\n",
    "    # Simple imputation - in production, use more sophisticated methods\n",
    "    for col in missing_features.index:\n",
    "        if new_data[col].dtype in ['int64', 'float64']:\n",
    "            new_data[col].fillna(new_data[col].median(), inplace=True)\n",
    "        else:\n",
    "            new_data[col].fillna(new_data[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Missing values imputed\")\n",
    "else:\n",
    "    print(f\"‚úÖ No missing values\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nüìä Data Types:\")\n",
    "print(new_data[required_features].dtypes.value_counts())\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5974f7-f825-45b2-bc6c-d1de3383d68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 8. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa236765-5606-42b5-893c-f8bba6c6d12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(config)\n",
    "\n",
    "# Create features (same as training)\n",
    "print(f\"\\nüîß Creating features...\")\n",
    "new_data_featured = feature_engineer.create_features(new_data)\n",
    "\n",
    "print(f\"‚úÖ Features created\")\n",
    "print(f\"  ‚Ä¢ Original features: {len(new_data.columns)}\")\n",
    "print(f\"  ‚Ä¢ New features: {len(new_data_featured.columns)}\")\n",
    "\n",
    "# Show new features\n",
    "original_cols = set(new_data.columns)\n",
    "new_cols = set(new_data_featured.columns) - original_cols\n",
    "\n",
    "if new_cols:\n",
    "    print(f\"\\nüìã Engineered Features:\")\n",
    "    for col in sorted(new_cols):\n",
    "        print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c588387-94fd-4bb2-8435-66a2aa4c4032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features (remove target if present)\n",
    "if target_col in new_data_featured.columns:\n",
    "    X_new = new_data_featured.drop(columns=[target_col])\n",
    "    y_actual = new_data_featured[target_col]\n",
    "    has_actuals = True\n",
    "    print(f\"\\nüìä Target column found - will compare predictions with actuals\")\n",
    "else:\n",
    "    X_new = new_data_featured.copy()\n",
    "    y_actual = None\n",
    "    has_actuals = False\n",
    "    print(f\"\\nüìä No target column - generating predictions only\")\n",
    "\n",
    "# Encode categorical features\n",
    "print(f\"\\nüîß Encoding categorical features...\")\n",
    "X_encoded = X_new.copy()\n",
    "\n",
    "for col, encoder in label_encoders.items():\n",
    "    if col in X_encoded.columns:\n",
    "        # Handle unseen categories\n",
    "        X_encoded[col] = X_encoded[col].apply(\n",
    "            lambda x: x if x in encoder.classes_ else encoder.classes_[0]\n",
    "        )\n",
    "        X_encoded[col] = encoder.transform(X_encoded[col])\n",
    "\n",
    "print(f\"‚úÖ Categorical features encoded\")\n",
    "\n",
    "# Ensure correct feature order\n",
    "X_encoded = X_encoded[scaler.feature_names_in_]\n",
    "\n",
    "print(f\"\\nüîß Scaling features...\")\n",
    "X_scaled = scaler.transform(X_encoded)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=scaler.feature_names_in_, index=X_encoded.index)\n",
    "\n",
    "print(f\"‚úÖ Features scaled\")\n",
    "print(f\"  ‚Ä¢ Final feature count: {X_scaled_df.shape[1]}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbbaefc-10a2-4803-943e-327ac5d3f973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 10. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5141dc8d-dbe8-486c-ac9f-9ebd5c92fb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make predictions\n",
    "print(f\"\\nüîÆ Making predictions...\")\n",
    "predictions = production_model.predict(X_scaled_df)\n",
    "\n",
    "print(f\"‚úÖ Predictions generated\")\n",
    "print(f\"  ‚Ä¢ Total predictions: {len(predictions)}\")\n",
    "\n",
    "# Calculate prediction statistics\n",
    "pred_stats = {\n",
    "    'mean': predictions.mean(),\n",
    "    'median': np.median(predictions),\n",
    "    'std': predictions.std(),\n",
    "    'min': predictions.min(),\n",
    "    'max': predictions.max(),\n",
    "    'q25': np.percentile(predictions, 25),\n",
    "    'q75': np.percentile(predictions, 75)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean:   ${pred_stats['mean']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Median: ${pred_stats['median']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Std:    ${pred_stats['std']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Min:    ${pred_stats['min']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Max:    ${pred_stats['max']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Q25:    ${pred_stats['q25']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Q75:    ${pred_stats['q75']:,.2f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc13c7e8-f70f-4bd5-8184-59a5f9ef8cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 11. Calculate Prediction Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75859987-7d70-4686-98ea-dcdfa1c655d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CALCULATING CONFIDENCE INTERVALS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For tree-based models, we can estimate uncertainty using prediction variance\n",
    "# For linear models, we use residual standard error\n",
    "\n",
    "model_type = type(production_model).__name__\n",
    "\n",
    "if hasattr(production_model, 'estimators_'):\n",
    "    # Ensemble model - use prediction variance across trees\n",
    "    print(f\"\\nüîß Using ensemble variance for confidence intervals...\")\n",
    "    \n",
    "    # Get predictions from all estimators\n",
    "    all_predictions = np.array([tree.predict(X_scaled_df) for tree in production_model.estimators_])\n",
    "    \n",
    "    # Calculate standard deviation across predictions\n",
    "    prediction_std = all_predictions.std(axis=0)\n",
    "    \n",
    "    # 95% confidence interval (¬±1.96 * std)\n",
    "    confidence_lower = predictions - 1.96 * prediction_std\n",
    "    confidence_upper = predictions + 1.96 * prediction_std\n",
    "    \n",
    "    print(f\"‚úÖ Confidence intervals calculated using ensemble variance\")\n",
    "    \n",
    "else:\n",
    "    # Non-ensemble model - use fixed percentage based on model performance\n",
    "    print(f\"\\nüîß Using model error for confidence intervals...\")\n",
    "    \n",
    "    # Use model's test RMSE as uncertainty estimate\n",
    "    model_rmse = metrics.get('test_rmse', predictions.std() * 0.2)\n",
    "    \n",
    "    # 95% confidence interval (¬±1.96 * RMSE)\n",
    "    confidence_lower = predictions - 1.96 * model_rmse\n",
    "    confidence_upper = predictions + 1.96 * model_rmse\n",
    "    \n",
    "    print(f\"‚úÖ Confidence intervals calculated using RMSE: ${model_rmse:,.2f}\")\n",
    "\n",
    "# Calculate interval width\n",
    "interval_width = confidence_upper - confidence_lower\n",
    "\n",
    "print(f\"\\nüìä Confidence Interval Statistics:\")\n",
    "print(f\"  ‚Ä¢ Mean Width: ${interval_width.mean():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Median Width: ${np.median(interval_width):,.2f}\")\n",
    "print(f\"  ‚Ä¢ Min Width: ${interval_width.min():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Max Width: ${interval_width.max():,.2f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff60777f-9118-4a63-9264-67c02a921427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 12. Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71394733-ebfd-4fce-9e2b-c9b4349e71f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CREATING RESULTS DATAFRAME\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = new_data.copy()\n",
    "results_df['Predicted_Price'] = predictions\n",
    "results_df['Confidence_Lower_95'] = confidence_lower\n",
    "results_df['Confidence_Upper_95'] = confidence_upper\n",
    "results_df['Confidence_Interval_Width'] = interval_width\n",
    "results_df['Prediction_Timestamp'] = datetime.now()\n",
    "results_df['Model_Version'] = production_version.version\n",
    "\n",
    "# Add actual vs predicted comparison if actuals available\n",
    "if has_actuals:\n",
    "    results_df['Actual_Price'] = y_actual.values\n",
    "    results_df['Prediction_Error'] = results_df['Actual_Price'] - results_df['Predicted_Price']\n",
    "    results_df['Absolute_Error'] = np.abs(results_df['Prediction_Error'])\n",
    "    results_df['Percentage_Error'] = (results_df['Prediction_Error'] / results_df['Actual_Price'] * 100)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results with actual comparisons created\")\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, predictions))\n",
    "    mae = mean_absolute_error(y_actual, predictions)\n",
    "    r2 = r2_score(y_actual, predictions)\n",
    "    \n",
    "    print(f\"\\nüìä Prediction Accuracy:\")\n",
    "    print(f\"  ‚Ä¢ RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ MAE:  ${mae:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Results created (predictions only)\")\n",
    "\n",
    "print(f\"\\nüìã Results Preview:\")\n",
    "display_cols = ['Predicted_Price', 'Confidence_Lower_95', 'Confidence_Upper_95']\n",
    "if has_actuals:\n",
    "    display_cols = ['Actual_Price'] + display_cols + ['Prediction_Error', 'Percentage_Error']\n",
    "\n",
    "safe_display(results_df[display_cols].head(10))\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf79e8ea-b62f-41ec-a89c-be0f28b0eea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 13. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b1431d-7d22-4785-a30f-9f5122829d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VISUALIZING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if has_actuals:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Actual vs Predicted\n",
    "    axes[0, 0].scatter(y_actual, predictions, alpha=0.6, s=50)\n",
    "    min_val = min(y_actual.min(), predictions.min())\n",
    "    max_val = max(y_actual.max(), predictions.max())\n",
    "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Actual Price ($)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title(f'Actual vs Predicted (R¬≤ = {r2:.4f})', fontsize=13, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Prediction errors\n",
    "    axes[0, 1].hist(results_df['Prediction_Error'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Prediction Error ($)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title(f'Prediction Error Distribution (MAE = ${mae:,.2f})', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Predictions with confidence intervals\n",
    "    sorted_idx = np.argsort(predictions)\n",
    "    axes[1, 0].plot(range(len(predictions)), predictions[sorted_idx], 'b-', label='Prediction', linewidth=2)\n",
    "    axes[1, 0].fill_between(range(len(predictions)), \n",
    "                            confidence_lower[sorted_idx], \n",
    "                            confidence_upper[sorted_idx], \n",
    "                            alpha=0.3, label='95% Confidence Interval')\n",
    "    axes[1, 0].scatter(range(len(predictions)), y_actual.values[sorted_idx], \n",
    "                      alpha=0.5, s=20, color='red', label='Actual', zorder=5)\n",
    "    axes[1, 0].set_xlabel('Sample (sorted by prediction)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Predictions with Confidence Intervals', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Percentage error distribution\n",
    "    axes[1, 1].hist(results_df['Percentage_Error'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1, 1].set_xlabel('Percentage Error (%)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Percentage Error Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "else:\n",
    "    # Predictions only visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    axes[0].hist(predictions, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=predictions.mean(), color='r', linestyle='--', lw=2, label=f'Mean: ${predictions.mean():,.2f}')\n",
    "    axes[0].set_xlabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Prediction Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. Predictions with confidence intervals\n",
    "    sorted_idx = np.argsort(predictions)\n",
    "    axes[1].plot(range(len(predictions)), predictions[sorted_idx], 'b-', label='Prediction', linewidth=2)\n",
    "    axes[1].fill_between(range(len(predictions)), \n",
    "                        confidence_lower[sorted_idx], \n",
    "                        confidence_upper[sorted_idx], \n",
    "                        alpha=0.3, label='95% Confidence Interval')\n",
    "    axes[1].set_xlabel('Sample (sorted by prediction)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Price ($)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Predictions with Confidence Intervals', fontsize=13, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualizations complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16ca6e4-568a-4001-a013-3d6328fed5d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 14. Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab78fe96-0d0c-43d1-a201-b052c1ebdc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SAVING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save predictions\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "predictions_path = f\"{processed_path}batch_predictions_{timestamp}.csv\"\n",
    "\n",
    "results_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions saved\")\n",
    "print(f\"  ‚Ä¢ File: {predictions_path}\")\n",
    "print(f\"  ‚Ä¢ Records: {len(results_df)}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_name': model_registry_name,\n",
    "    'model_version': production_version.version,\n",
    "    'model_type': model_type,\n",
    "    'total_predictions': len(predictions),\n",
    "    \n",
    "    'prediction_statistics': {\n",
    "        'mean': float(pred_stats['mean']),\n",
    "        'median': float(pred_stats['median']),\n",
    "        'std': float(pred_stats['std']),\n",
    "        'min': float(pred_stats['min']),\n",
    "        'max': float(pred_stats['max'])\n",
    "    },\n",
    "    \n",
    "    'confidence_intervals': {\n",
    "        'mean_width': float(interval_width.mean()),\n",
    "        'median_width': float(np.median(interval_width))\n",
    "    }\n",
    "}\n",
    "\n",
    "if has_actuals:\n",
    "    summary_report['accuracy_metrics'] = {\n",
    "        'rmse': float(rmse),\n",
    "        'mae': float(mae),\n",
    "        'r2': float(r2)\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{processed_path}batch_predictions_summary_{timestamp}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Summary report saved\")\n",
    "print(f\"  ‚Ä¢ File: {summary_path}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1f984f-8471-46ed-b0bf-11e0f2d89c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 15. Log to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c887a537-2df8-4fb6-9750-5476fdac7d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOGGING TO MLFLOW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"batch_predictions_{timestamp}\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_version\", production_version.version)\n",
    "    mlflow.log_param(\"num_predictions\", len(predictions))\n",
    "    mlflow.log_param(\"has_actuals\", has_actuals)\n",
    "    \n",
    "    # Log prediction statistics\n",
    "    mlflow.log_metrics({\n",
    "        \"pred_mean\": pred_stats['mean'],\n",
    "        \"pred_median\": pred_stats['median'],\n",
    "        \"pred_std\": pred_stats['std'],\n",
    "        \"pred_min\": pred_stats['min'],\n",
    "        \"pred_max\": pred_stats['max']\n",
    "    })\n",
    "    \n",
    "    # Log confidence interval statistics\n",
    "    mlflow.log_metrics({\n",
    "        \"ci_mean_width\": interval_width.mean(),\n",
    "        \"ci_median_width\": np.median(interval_width)\n",
    "    })\n",
    "    \n",
    "    # Log accuracy metrics if available\n",
    "    if has_actuals:\n",
    "        mlflow.log_metrics({\n",
    "            \"batch_rmse\": rmse,\n",
    "            \"batch_mae\": mae,\n",
    "            \"batch_r2\": r2\n",
    "        })\n",
    "    \n",
    "    # Log artifacts\n",
    "    mlflow.log_artifact(predictions_path)\n",
    "    mlflow.log_artifact(summary_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Logged to MLflow\")\n",
    "    print(f\"  ‚Ä¢ Run name: batch_predictions_{timestamp}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0a12d0-2a61-4765-a4c6-5551d14716bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 16. Batch Prediction Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7bcba9-f340-4117-a285-9009f5efe93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BATCH PREDICTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÖ Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nü§ñ Model Information:\")\n",
    "print(f\"  ‚Ä¢ Model: {model_registry_name}\")\n",
    "print(f\"  ‚Ä¢ Version: {production_version.version}\")\n",
    "print(f\"  ‚Ä¢ Type: {model_type}\")\n",
    "\n",
    "print(f\"\\nüìä Predictions:\")\n",
    "print(f\"  ‚Ä¢ Total: {len(predictions)}\")\n",
    "print(f\"  ‚Ä¢ Mean: ${pred_stats['mean']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Range: ${pred_stats['min']:,.2f} - ${pred_stats['max']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìè Confidence Intervals:\")\n",
    "print(f\"  ‚Ä¢ Mean Width: ${interval_width.mean():,.2f}\")\n",
    "print(f\"  ‚Ä¢ Coverage: 95%\")\n",
    "\n",
    "if has_actuals:\n",
    "    print(f\"\\nüéØ Accuracy:\")\n",
    "    print(f\"  ‚Ä¢ RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ MAE:  ${mae:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"  ‚Ä¢ batch_predictions_{timestamp}.csv\")\n",
    "print(f\"  ‚Ä¢ batch_predictions_summary_{timestamp}.json\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  ‚Ä¢ Review predictions in saved CSV file\")\n",
    "print(f\"  ‚Ä¢ Share results with stakeholders\")\n",
    "print(f\"  ‚Ä¢ Monitor prediction quality over time\")\n",
    "if has_actuals:\n",
    "    print(f\"  ‚Ä¢ Compare with monitoring thresholds\")\n",
    "    print(f\"  ‚Ä¢ Trigger retraining if accuracy degrades\")\n",
    "\n",
    "print(f\"\\n‚úÖ Batch prediction pipeline complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31cd4c03-9bdd-4b99-bd9b-8de3f513ba85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_batch_predictions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
