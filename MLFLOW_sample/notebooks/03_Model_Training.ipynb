{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d01a97-d423-467e-be7c-fdcb74b8d5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC # 03. Model Training & Evaluation\n",
    "# MAGIC \n",
    "# MAGIC **Production-Grade ML Pipeline - House Price Prediction**\n",
    "# MAGIC \n",
    "# MAGIC This notebook:\n",
    "# MAGIC - Loads processed training data\n",
    "# MAGIC - Trains multiple regression models\n",
    "# MAGIC - Performs hyperparameter tuning with GridSearchCV\n",
    "# MAGIC - Tracks all experiments with MLflow\n",
    "# MAGIC - Evaluates and compares model performance\n",
    "# MAGIC - Registers the best model to MLflow Model Registry\n",
    "# MAGIC \n",
    "# MAGIC ---\n",
    "# MAGIC **Author:** Satish  \n",
    "# MAGIC **Date:** 2026-01-17  \n",
    "# MAGIC **MLflow Experiment:** House Price Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d56229a-1d57-4846-9aff-8eb24620211e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9a9426-e4d1-497e-9366-b66fcb4b951b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dceb9d8-57be-478b-b18c-3632ebc2acb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e768f8-f856-47a8-97d6-96002bbc93f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "project_root = '/Workspace/COMM - Commercial Analytics (CMAN)/MMM Quattro 2025/Satish/MLFLOW_sample'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566848f8-4ac7-49aa-b45b-c57a847a55f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Custom imports\n",
    "from src.utils import (\n",
    "    ConfigLoader, \n",
    "    DataLoader, \n",
    "    MLflowLogger,\n",
    "    safe_display\n",
    ")\n",
    "from src.model import ModelTrainer, ModelEvaluator\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82dcf1be-e91c-4411-a706-e132ed6300a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Load Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ae4778-7da4-4f5d-adb8-cab7c11cbb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load config\n",
    "config_loader = ConfigLoader()\n",
    "config = config_loader.load_config(f'{project_root}/config/config.yaml')\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration loaded\")\n",
    "print(f\"  ‚Ä¢ Project: {config['project']['name']}\")\n",
    "print(f\"  ‚Ä¢ Version: {config['project']['version']}\")\n",
    "print(f\"  ‚Ä¢ MLflow Experiment: {config['mlflow']['experiment_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ace090-8b42-4423-9b07-999c717213bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Setup MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3e0440-2257-4dc5-a5a1-b3395d2d5958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SETTING UP MLFLOW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = config['mlflow']['experiment_name']\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"\\n‚úÖ MLflow experiment set: {experiment_name}\")\n",
    "\n",
    "# Get experiment info\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"  ‚Ä¢ Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"  ‚Ä¢ Artifact Location: {experiment.artifact_location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a028f6-395e-4650-8c01-04a4e8727e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Load Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31ebe02-efab-44b3-bd22-8a3e457e323a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_path = config['data']['processed_path']\n",
    "\n",
    "# Load datasets\n",
    "X_train = DataLoader.load_csv(f'{processed_path}X_train.csv')\n",
    "X_test = DataLoader.load_csv(f'{processed_path}X_test.csv')\n",
    "y_train = DataLoader.load_csv(f'{processed_path}y_train.csv').squeeze()\n",
    "y_test = DataLoader.load_csv(f'{processed_path}y_test.csv').squeeze()\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully\")\n",
    "print(f\"  ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"  ‚Ä¢ y_train: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ y_test: {y_test.shape}\")\n",
    "\n",
    "# Show feature names\n",
    "print(f\"\\nüìã Features ({len(X_train.columns)}):\")\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show target statistics\n",
    "print(f\"\\nüìä Target Variable Statistics:\")\n",
    "print(f\"  Training Set:\")\n",
    "print(f\"    Mean:   ${y_train.mean():,.2f}\")\n",
    "print(f\"    Median: ${y_train.median():,.2f}\")\n",
    "print(f\"    Std:    ${y_train.std():,.2f}\")\n",
    "print(f\"    Range:  [${y_train.min():,.2f}, ${y_train.max():,.2f}]\")\n",
    "print(f\"\\n  Test Set:\")\n",
    "print(f\"    Mean:   ${y_test.mean():,.2f}\")\n",
    "print(f\"    Median: ${y_test.median():,.2f}\")\n",
    "print(f\"    Std:    ${y_test.std():,.2f}\")\n",
    "print(f\"    Range:  [${y_test.min():,.2f}, ${y_test.max():,.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a6d925-049c-4cf9-b88f-80aea208ec61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 5. Initialize Model Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d893ba96-0f81-466e-b91e-6becf0bb4b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING MODEL TRAINER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "print(f\"\\n‚úÖ ModelTrainer initialized\")\n",
    "print(f\"  ‚Ä¢ Available models: {len(config['models'])}\")\n",
    "print(f\"  ‚Ä¢ CV folds: {config['training']['cv_folds']}\")\n",
    "print(f\"  ‚Ä¢ Scoring metric: {config['training']['scoring']}\")\n",
    "\n",
    "# Show configured models\n",
    "print(f\"\\nüìã Configured Models:\")\n",
    "for i, model_name in enumerate(config['models'].keys(), 1):\n",
    "    print(f\"  {i}. {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff3360-2f8e-4b05-ba22-15a0440878f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f7634-eca5-459b-9d1c-723afa4304b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Models to train\n",
    "models_to_train = ['linear_regression', 'ridge', 'lasso', 'random_forest', 'gradient_boosting']\n",
    "\n",
    "print(f\"\\nüöÄ Training {len(models_to_train)} models with hyperparameter tuning...\\n\")\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Train model with hyperparameter tuning\n",
    "        best_model, best_params, cv_results = trainer.train_model(\n",
    "            model_name=model_name,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            tune_hyperparameters=True\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            'mae': mean_absolute_error(y_train, y_train_pred),\n",
    "            'r2': r2_score(y_train, y_train_pred)\n",
    "        }\n",
    "        \n",
    "        test_metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'mae': mean_absolute_error(y_test, y_test_pred),\n",
    "            'r2': r2_score(y_test, y_test_pred)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': best_model,\n",
    "            'params': best_params,\n",
    "            'cv_results': cv_results,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'y_train_pred': y_train_pred,\n",
    "            'y_test_pred': y_test_pred\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n‚úÖ {model_name.upper()} Training Complete\")\n",
    "        print(f\"\\nüìä Best Parameters:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüìà Training Metrics:\")\n",
    "        print(f\"  ‚Ä¢ RMSE: ${train_metrics['rmse']:,.2f}\")\n",
    "        print(f\"  ‚Ä¢ MAE:  ${train_metrics['mae']:,.2f}\")\n",
    "        print(f\"  ‚Ä¢ R¬≤:   {train_metrics['r2']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìâ Test Metrics:\")\n",
    "        print(f\"  ‚Ä¢ RMSE: ${test_metrics['rmse']:,.2f}\")\n",
    "        print(f\"  ‚Ä¢ MAE:  ${test_metrics['mae']:,.2f}\")\n",
    "        print(f\"  ‚Ä¢ R¬≤:   {test_metrics['r2']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error training {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e83ba8e-6583-4d3b-a8fe-2eb858292e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Compare Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165e9a00-ee75-4bfa-9fd7-d63df3320879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name.replace('_', ' ').title(),\n",
    "        'Train RMSE': result['train_metrics']['rmse'],\n",
    "        'Test RMSE': result['test_metrics']['rmse'],\n",
    "        'Train MAE': result['train_metrics']['mae'],\n",
    "        'Test MAE': result['test_metrics']['mae'],\n",
    "        'Train R¬≤': result['train_metrics']['r2'],\n",
    "        'Test R¬≤': result['test_metrics']['r2'],\n",
    "        'Overfit (RMSE)': result['train_metrics']['rmse'] - result['test_metrics']['rmse']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test RMSE')\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "safe_display(comparison_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model'].lower().replace(' ', '_')\n",
    "print(f\"\\nüèÜ Best Model: {comparison_df.iloc[0]['Model']}\")\n",
    "print(f\"  ‚Ä¢ Test RMSE: ${comparison_df.iloc[0]['Test RMSE']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Test R¬≤: {comparison_df.iloc[0]['Test R¬≤']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc26a9c-ed4e-4d5b-9700-39dba3136c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 8. Visualize Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310f737e-b8db-4fa0-83df-91ac3cab14ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALIZING MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. RMSE Comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison_df['Train RMSE'], width, label='Train RMSE', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n",
    "ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('RMSE ($)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. R¬≤ Comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x - width/2, comparison_df['Train R¬≤'], width, label='Train R¬≤', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison_df['Test R¬≤'], width, label='Test R¬≤', alpha=0.8)\n",
    "ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('R¬≤ Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. MAE Comparison\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x - width/2, comparison_df['Train MAE'], width, label='Train MAE', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison_df['Test MAE'], width, label='Test MAE', alpha=0.8)\n",
    "ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('MAE ($)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Overfitting Analysis\n",
    "ax = axes[1, 1]\n",
    "colors = ['green' if x < 0 else 'red' for x in comparison_df['Overfit (RMSE)']]\n",
    "ax.barh(comparison_df['Model'], comparison_df['Overfit (RMSE)'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Train RMSE - Test RMSE ($)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Overfitting Analysis (Closer to 0 is Better)', fontsize=12, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison visualizations complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9836c2-c43e-42e9-aeca-fbd5de859d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Detailed Evaluation of Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbbf37a-4be1-4170-962d-513240dddc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"DETAILED EVALUATION: {best_model_name.upper()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best model results\n",
    "best_result = results[best_model_name]\n",
    "best_model = best_result['model']\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(config)\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_results = evaluator.evaluate_model(\n",
    "    model=best_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    model_name=best_model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Detailed evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996800f6-6d64-469a-b691-cb079e8fea00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 10. Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c065625-5845-435c-91d5-8ef9e6ad8bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance (works for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 Most Important Features:\")\n",
    "    safe_display(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(15, len(feature_importance))\n",
    "    plt.barh(range(top_n), feature_importance['Importance'].head(top_n))\n",
    "    plt.yticks(range(top_n), feature_importance['Feature'].head(top_n))\n",
    "    plt.xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=11, fontweight='bold')\n",
    "    plt.title(f'Top {top_n} Feature Importances - {best_model_name.title()}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models, show coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': best_model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 Features by Coefficient Magnitude:\")\n",
    "    safe_display(feature_importance.head(10))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(15, len(feature_importance))\n",
    "    colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient'].head(top_n)]\n",
    "    plt.barh(range(top_n), feature_importance['Coefficient'].head(top_n), color=colors, alpha=0.7)\n",
    "    plt.yticks(range(top_n), feature_importance['Feature'].head(top_n))\n",
    "    plt.xlabel('Coefficient', fontsize=11, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=11, fontweight='bold')\n",
    "    plt.title(f'Top {top_n} Feature Coefficients - {best_model_name.title()}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa46aed-c9cd-4a93-94c1-b8dc643b9603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Register Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9104127f-0379-4ce7-b0e6-02323c7aa975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 11. Register Best Model\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGISTERING BEST MODEL TO MLFLOW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "model_name_registry = config['mlflow']['model_registry_name']\n",
    "\n",
    "print(f\"\\nüìã Registration Details:\")\n",
    "print(f\"  ‚Ä¢ Registry Name: {model_name_registry}\")\n",
    "print(f\"  ‚Ä¢ Best Model: {best_model_name}\")\n",
    "\n",
    "try:\n",
    "    # Log and register model\n",
    "    with mlflow.start_run(run_name=f\"best_model_{best_model_name}\"):\n",
    "        # Log parameters\n",
    "        MLflowLogger.log_params_from_dict(best_result['params'])\n",
    "        \n",
    "        # Log metrics\n",
    "        MLflowLogger.log_metrics_from_dict({\n",
    "            f\"train_{k}\": v for k, v in best_result['train_metrics'].items()\n",
    "        })\n",
    "        MLflowLogger.log_metrics_from_dict({\n",
    "            f\"test_{k}\": v for k, v in best_result['test_metrics'].items()\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=model_name_registry\n",
    "        )\n",
    "        \n",
    "        # Log comparison DataFrame\n",
    "        MLflowLogger.log_dataframe_as_artifact(comparison_df, \"model_comparison.csv\")\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "    print(f\"\\n‚úÖ Model registered successfully!\")\n",
    "    print(f\"  ‚Ä¢ Model Name: {model_name_registry}\")\n",
    "    print(f\"  ‚Ä¢ Run ID: {run_id}\")\n",
    "    print(f\"  ‚Ä¢ Best Model: {best_model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error registering model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477fe797-fd06-4b64-9b60-5eb41bf3247c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71200c0c-cf7e-4fb4-a9ef-1b4db094b6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 12. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a230c8-3b1d-4b78-bf79-155fcc6b1a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Models Trained: {len(results)}\")\n",
    "print(f\"üèÜ Best Model: {best_model_name.title()}\")\n",
    "print(f\"\\nüìà Best Model Performance:\")\n",
    "print(f\"  ‚Ä¢ Test RMSE: ${best_result['test_metrics']['rmse']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Test MAE:  ${best_result['test_metrics']['mae']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Test R¬≤:   {best_result['test_metrics']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nüì¶ MLflow:\")\n",
    "print(f\"  ‚Ä¢ Experiment: {experiment_name}\")\n",
    "print(f\"  ‚Ä¢ Registered Model: {model_name_registry}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model training complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e87fb4-29d2-4191-8052-1d42eaaee505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ---\n",
    "# MAGIC ## ‚úÖ Next Steps\n",
    "# MAGIC \n",
    "# MAGIC 1. **Model Deployment**: Deploy the best model for inference\n",
    "# MAGIC 2. **Monitoring**: Set up model monitoring and drift detection\n",
    "# MAGIC 3. **Retraining**: Schedule periodic model retraining\n",
    "# MAGIC 4. **A/B Testing**: Compare new models against production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b678ec-b4bf-43ea-a782-8fbcf4d3fcbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Model_Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
