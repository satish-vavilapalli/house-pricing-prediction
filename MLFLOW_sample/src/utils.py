# src/utils.py - DATABRICKS PRODUCTION READY

import yaml
import logging
import pandas as pd
from pathlib import Path
import mlflow
from typing import Dict, Any, List, Union, Optional
import json
import tempfile

# -------------------------------------------------------------------
# Logging setup
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# -------------------------------------------------------------------
# Config loader
# -------------------------------------------------------------------
class ConfigLoader:
    """Load and manage configuration."""

    @staticmethod
    def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:
        """Load YAML configuration file."""
        try:
            config_path = Path(config_path)
            with config_path.open("r") as file:
                config = yaml.safe_load(file)
            logger.info(f"âœ… Config loaded: {config_path}")
            return config
        except FileNotFoundError:
            logger.error(f"âŒ Config file not found: {config_path}")
            raise
        except yaml.YAMLError as e:
            logger.error(f"âŒ YAML parsing error in {config_path}: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"âŒ Config error {config_path}: {str(e)}")
            raise


# -------------------------------------------------------------------
# Data IO helpers
# -------------------------------------------------------------------
class DataLoader:
    """Handle data loading operations."""

    @staticmethod
    def load_csv(file_path: Union[str, Path]) -> pd.DataFrame:
        """Load CSV file into a DataFrame."""
        try:
            file_path = Path(file_path)
            df = pd.read_csv(file_path)
            logger.info(f"ðŸ“Š Loaded {file_path.name}: {df.shape}")
            return df
        except FileNotFoundError:
            logger.error(f"âŒ File not found: {file_path}")
            raise
        except pd.errors.EmptyDataError:
            logger.error(f"âŒ Empty CSV file: {file_path}")
            raise
        except Exception as e:
            logger.error(f"âŒ Load failed {file_path}: {str(e)}")
            raise

    @staticmethod
    def save_csv(df: pd.DataFrame, file_path: Union[str, Path]) -> None:
        """Save DataFrame to CSV."""
        try:
            file_path = Path(file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            df.to_csv(file_path, index=False)
            logger.info(f"ðŸ’¾ Saved {file_path.name}: {df.shape}")
        except Exception as e:
            logger.error(f"âŒ Save failed {file_path}: {str(e)}")
            raise


# -------------------------------------------------------------------
# Databricks MLflow (SIMPLE & SAFE)
# -------------------------------------------------------------------
class MLflowLogger:
    """Databricks MLflow logging utilities."""

    @staticmethod
    def log_params_from_dict(params: Dict[str, Any]) -> None:
        """Log parameters from a dictionary."""
        if not params:
            logger.warning("âš ï¸ No params to log")
            return
        try:
            # MLflow has a limit on param value length (500 chars)
            # Truncate long values
            truncated_params = {
                k: str(v)[:500] if len(str(v)) > 500 else v 
                for k, v in params.items()
            }
            mlflow.log_params(truncated_params)
            logger.info(f"ðŸ“ Logged {len(params)} params")
        except mlflow.exceptions.MlflowException as e:
            logger.warning(f"âš ï¸ MLflow param logging: {str(e)}")
        except Exception as e:
            logger.error(f"âŒ Param logging: {str(e)}")
            raise

    @staticmethod
    def log_metrics_from_dict(metrics: Dict[str, float]) -> None:
        """Log metrics from a dictionary."""
        if not metrics:
            logger.warning("âš ï¸ No metrics to log")
            return
        try:
            # Ensure all values are numeric
            numeric_metrics = {
                k: float(v) for k, v in metrics.items() 
                if isinstance(v, (int, float)) and not pd.isna(v)
            }
            if numeric_metrics:
                mlflow.log_metrics(numeric_metrics)
                logger.info(f"ðŸ“Š Logged {len(numeric_metrics)} metrics")
            else:
                logger.warning("âš ï¸ No valid numeric metrics to log")
        except mlflow.exceptions.MlflowException as e:
            logger.warning(f"âš ï¸ MLflow metric logging: {str(e)}")
        except Exception as e:
            logger.error(f"âŒ Metric logging: {str(e)}")
            raise

    @staticmethod
    def log_artifact_from_dict(data: Dict[str, Any], filename: str) -> None:
        """Log dictionary as JSON artifact (tempdir safe)."""
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                tmp_path = Path(tmpdir) / filename
                with tmp_path.open("w") as f:
                    json.dump(data, f, indent=2, default=str)  # default=str handles non-serializable objects
                mlflow.log_artifact(str(tmp_path))
            logger.info(f"ðŸ“¦ Artifact logged: {filename}")
        except Exception as e:
            logger.error(f"âŒ Artifact {filename}: {str(e)}")
            raise

    @staticmethod
    def log_dataframe_as_artifact(df: pd.DataFrame, filename: str) -> None:
        """Log DataFrame as CSV artifact."""
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                tmp_path = Path(tmpdir) / filename
                df.to_csv(tmp_path, index=False)
                mlflow.log_artifact(str(tmp_path))
            logger.info(f"ðŸ“¦ DataFrame artifact logged: {filename}")
        except Exception as e:
            logger.error(f"âŒ DataFrame artifact {filename}: {str(e)}")
            raise


# -------------------------------------------------------------------
# Data validation
# -------------------------------------------------------------------
class DataValidator:
    """Validate data quality."""

    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> None:
        """Fail-fast validation for required columns."""
        if df is None or df.empty:
            msg = "âŒ DataFrame is None or empty"
            logger.error(msg)
            raise ValueError(msg)
        
        missing_cols = set(required_columns) - set(df.columns)
        if missing_cols:
            msg = f"âŒ Missing columns: {missing_cols}"
            logger.error(msg)
            raise ValueError(msg)
        logger.info("âœ… All required columns validated")

    @staticmethod
    def check_missing_values(df: pd.DataFrame) -> pd.Series:
        """Check and log missing values."""
        missing = df.isnull().sum()
        total_missing = missing.sum()
        
        if total_missing > 0:
            missing_pct = (total_missing / (df.shape[0] * df.shape[1])) * 100
            logger.warning(f"âš ï¸ Missing values: {total_missing} ({missing_pct:.2f}%)")
            logger.warning(f"Columns with missing:\n{missing[missing > 0]}")
        else:
            logger.info("âœ… No missing values")
        return missing

    @staticmethod
    def check_data_types(df: pd.DataFrame) -> pd.Series:
        """Log data types."""
        dtypes = df.dtypes
        logger.info(f"ðŸ“‹ Data types:\n{dtypes}")
        return dtypes

    @staticmethod
    def check_duplicates(df: pd.DataFrame) -> int:
        """Check for duplicate rows."""
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            logger.warning(f"âš ï¸ Found {duplicates} duplicate rows")
        else:
            logger.info("âœ… No duplicate rows")
        return duplicates


# -------------------------------------------------------------------
# Databricks MLflow Setup (ULTRA-SIMPLE)
# -------------------------------------------------------------------
def setup_mlflow_databricks(config: Dict[str, Any]) -> None:
    """Databricks-only MLflow setup (no tracking_uri needed)."""
    try:
        mlflow_config = config.get('mlflow', {})
        experiment_name = mlflow_config.get('experiment_name')
        
        if not experiment_name:
            logger.warning("âš ï¸ No 'experiment_name' in config.mlflow - using default")
            return
        
        # Set experiment (creates if doesn't exist)
        mlflow.set_experiment(experiment_name)
        logger.info(f"âœ… MLflow experiment set: {experiment_name}")
        
        # Enable autologging if specified
        if mlflow_config.get('autolog', False):
            mlflow.sklearn.autolog()
            logger.info("âœ… MLflow autologging enabled")
            
    except Exception as e:
        logger.warning(f"âš ï¸ MLflow setup issue: {e}")


# -------------------------------------------------------------------
# Project helpers
# -------------------------------------------------------------------
def get_project_root() -> Path:
    """Get project root directory."""
    return Path(__file__).resolve().parent.parent


def log_dataset_summary(df: pd.DataFrame, config: Dict[str, Any]) -> None:
    """Log dataset info to MLflow."""
    try:
        target = config['preprocessing']['target']
        
        summary = {
            "dataset_rows": int(df.shape[0]),
            "dataset_columns": int(df.shape[1]),
            "memory_kb": float(df.memory_usage(deep=True).sum() / 1024),
        }
        
        # Add target statistics if target exists
        if target in df.columns:
            summary.update({
                "target_mean": float(df[target].mean()),
                "target_std": float(df[target].std()),
                "target_min": float(df[target].min()),
                "target_max": float(df[target].max()),
            })
        
        MLflowLogger.log_params_from_dict(summary)
        logger.info("ðŸ“Š Dataset summary logged to MLflow")
        
    except Exception as e:
        logger.warning(f"âš ï¸ Could not log dataset summary: {e}")


def create_run_name(model_name: str, timestamp: Optional[str] = None) -> str:
    """Create a standardized MLflow run name."""
    from datetime import datetime
    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{model_name}_{timestamp}"


def safe_display(df: pd.DataFrame, max_rows: int = 10) -> None:
    """Safely display DataFrame (handles empty DataFrames)."""
    if df is None or df.empty:
        print("ðŸ“­ Empty DataFrame - nothing to display")
        return
    
    try:
        # In Databricks, use display()
        display(df.head(max_rows))
    except NameError:
        # Fallback for non-Databricks environments
        print(df.head(max_rows))